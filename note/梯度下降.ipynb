{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4422bc61",
   "metadata": {},
   "source": [
    "求最优点时，穷举法计算量太大，分治法容易陷入局部最优点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860c772",
   "metadata": {},
   "source": [
    "梯度是上升最快的方向"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15695017",
   "metadata": {},
   "source": [
    "w = w - a * 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973354df",
   "metadata": {},
   "source": [
    "每次迭代时，都朝下降最快的方向前进一步。属贪心算法。可找到局部最优点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5756148",
   "metadata": {},
   "source": [
    "多次迭代，可找到全局最优点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8536eee",
   "metadata": {},
   "source": [
    "深度学习中，损失函数中很少有局部最优点，但有很多鞍点，即梯度为0的点。到达鞍点后，梯度下降无法继续前进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fd9de",
   "metadata": {},
   "source": [
    "实际工作中剃度下降很少用，用的最多的是随机梯度下降。\n",
    "随机梯度下降使用单个样本的损失对权重进行求导，然后更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac9e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
